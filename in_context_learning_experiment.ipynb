{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Importing necessary libraries\n", "import torch\n", "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "def load_model_and_tokenizer(model_name=\"t5-large\"):\n", "    \"\"\"Load the model and tokenizer for the specified model name.\"\"\"\n", "    try:\n", "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n", "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "        return model, tokenizer\n", "    except Exception as e:\n", "        print(f\"Error loading model and tokenizer: {e}\")\n", "        raise\n", "\n", "def freeze_model_parameters(model):\n", "    \"\"\"Freeze all parameters in the model except for prompt embeddings.\"\"\"\n", "    for param in model.parameters():\n", "        param.requires_grad = False\n", "\n", "def initialize_prompt_embedding(prompt_length, d_model):\n", "    \"\"\"Initialize learnable prompt embeddings.\"\"\"\n", "    return torch.nn.Parameter(torch.randn(prompt_length, d_model))\n", "\n", "def set_input_embeddings(model, prompt_embedding):\n", "    \"\"\"Set the model's input embeddings by concatenating prompt and original embeddings.\"\"\"\n", "    original_input_embeddings_weight = model.get_input_embeddings().weight\n", "    model.set_input_embeddings(torch.nn.Embedding.from_pretrained(\n", "        torch.cat([prompt_embedding, original_input_embeddings_weight], dim=0),\n", "        freeze=False\n", "    ))\n", "\n", "def preprocess_data(dataset, tokenizer, task):\n", "    \"\"\"Preprocess the dataset for a specific NLP task and create in-context prompts.\"\"\"\n", "    def create_in_context_prompt(example):\n", "        try:\n", "            if task == \"QA\":\n", "                in_context_examples = f\"Question: {example['question']}\\nAnswer: {example['answers']['text'][0]}\"\n", "                inputs = f\"{in_context_examples}\\nQuestion: {example['question']}\\nAnswer:\"\n", "                label = example['answers']['text'][0]\n", "            elif task == \"NLI\":\n", "                in_context_examples = f\"Premise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\nLabel: {example['label']}\"\n", "                inputs = f\"{in_context_examples}\\nPremise: {example['premise']}\\nHypothesis: {example['hypothesis']}\\nLabel:\"\n", "                label = example['label']\n", "            elif task == \"Classification\":\n", "                in_context_examples = f\"Question: {example['text']}\\nCategory: {example['label-coarse']}\"\n", "                inputs = f\"{in_context_examples}\\nQuestion: {example['text']}\\nCategory:\"\n", "                label = example['label-coarse']\n", "            else:\n", "                raise ValueError(\"Unsupported task type specified.\")\n", "            \n", "            # Tokenize inputs and labels\n", "            tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n", "            with tokenizer.as_target_tokenizer():\n", "                labels = tokenizer(str(label), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n", "            tokenized_inputs['labels'] = labels['input_ids']\n", "            return tokenized_inputs\n", "        except Exception as e:\n", "            print(f\"Error creating in-context prompt: {e}\")\n", "            raise\n", "\n", "    return dataset.map(lambda x: create_in_context_prompt(x), batched=False)\n", "\n", "def train_model(model, tokenized_datasets):\n", "    \"\"\"Train the model using the provided tokenized datasets.\"\"\"\n", "    training_args = TrainingArguments(\n", "        output_dir='./results',\n", "        evaluation_strategy=\"epoch\",\n", "        learning_rate=5e-5,\n", "        per_device_train_batch_size=8,\n", "        per_device_eval_batch_size=8,\n", "        num_train_epochs=3,\n", "        weight_decay=0.01,\n", "    )\n", "    \n", "    trainer = Trainer(\n", "        model=model,\n", "        args=training_args,\n", "        train_dataset=tokenized_datasets[\"SQuAD\"],\n", "        eval_dataset=tokenized_datasets[\"MultiNLI\"],\n", "    )\n", "\n", "    # Train the model\n", "    trainer.train()\n", "    # Evaluate the model\n", "    eval_results = trainer.evaluate()\n", "    print(f\"Evaluation results: {eval_results}\")\n", "\n", "def main():\n", "    model_name = \"t5-large\"\n", "    prompt_length = 20\n", "\n", "    # Load model and tokenizer\n", "    model, tokenizer = load_model_and_tokenizer(model_name)\n", "\n", "    # Freeze model parameters\n", "    freeze_model_parameters(model)\n", "\n", "    # Initialize and set prompt embeddings\n", "    prompt_embedding = initialize_prompt_embedding(prompt_length, model.config.d_model)\n", "    set_input_embeddings(model, prompt_embedding)\n", "\n", "    # Load datasets\n", "    datasets = {\n", "        \"SQuAD\": load_dataset(\"squad\"),\n", "        \"MultiNLI\": load_dataset(\"multi_nli\"),\n", "        \"TREC\": load_dataset(\"trec\")\n", "    }\n", "\n", "    # Preprocess datasets\n", "    tokenized_datasets = {\n", "        \"SQuAD\": preprocess_data(datasets[\"SQuAD\"]['train'], tokenizer, task=\"QA\"),\n", "        \"MultiNLI\": preprocess_data(datasets[\"MultiNLI\"]['train'], tokenizer, task=\"NLI\"),\n", "        \"TREC\": preprocess_data(datasets[\"TREC\"]['train'], tokenizer, task=\"Classification\")\n", "    }\n", "\n", "    # Train and evaluate the model\n", "    train_model(model, tokenized_datasets)\n", "\n", "if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}